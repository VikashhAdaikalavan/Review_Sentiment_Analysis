# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t2UXo2vq4fOkzOW5pT9tnhj6X3JHtqjK
"""

from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import nltk

import string
from nltk.corpus import stopwords



# Read in the data
df = pd.read_csv('Reviews.csv')



df.head()

nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

#cleaning the dataset
df.dropna(inplace=True)
#making all text small case and clearing punchuations
# Get the list of stop words and punctuation
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)

# Combine stop words and punctuation into a single set
waste_words = stop_words.union(punctuation)

def tokenize_and_remove_punctuation(text):
    tokens = nltk.word_tokenize(text.lower())  # Tokenize and convert to lowercase
    tokens = [word for word in tokens if word not in waste_words]  # Remove punctuation and stop words
    return tokens

# Apply the function to the 'review' column
df['review'] = df['review'].apply(tokenize_and_remove_punctuation)
df.head()

# encoding postive as 1 and negative as zero
labelencoder = LabelEncoder()
df['sentiment'] = labelencoder.fit_transform(df['sentiment'])
df.head()

lemmatizer = nltk.WordNetLemmatizer()

# Function to lemmatize a list of tokens
def lemmatize_tokens(tokens):
    return ' '.join([lemmatizer.lemmatize(token) for token in tokens])  # Lemmatize each token in the list

# Apply the function to the 'review' column (which already contains lists of tokens)
df['review'] = df['review'].apply(lemmatize_tokens)

df.head()

X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], random_state=0)

from sklearn.feature_extraction.text import CountVectorizer

# Fit the CountVectorizer to the training data
vect = CountVectorizer().fit(X_train)

#finding length of volacbulary
len(vect.get_feature_names_out())

#seeing the vocabulary
vect.get_feature_names_out()[100::1000]

#converting the data into a usable matrix
#we can see that the number of coloums in the matrix is the number of vocabulary words found earlier
X_train_vectorized = vect.transform(X_train)

X_train_vectorized
# sparse matrix is the output

# model building and training
from sklearn import naive_bayes

model = naive_bayes.MultinomialNB()

model.fit(X_train_vectorized,y_train)

y_predict = model.predict(vect.transform(X_test))

y_predict

# model evaluation
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score


print('AUC: ', roc_auc_score(y_test, y_predict))
print(f'F1 Score: ',f1_score(y_test, y_predict))

#not very suitable

from sklearn.feature_extraction.text import TfidfVectorizer
# min_df represents minimum number of doucments in which words must be in orer to be a part of the vocabulary
vect2 = TfidfVectorizer(min_df=5).fit(X_train)
len(vect2.get_feature_names_out())

"""As we can see the number of word in vocabulary is ignificantly reduced"""

from sklearn.linear_model import LogisticRegression
model2 = naive_bayes.MultinomialNB()
model2.fit(X_train_vectorized, y_train)

predictions = model2.predict_proba(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, y_predict))
print('F1 Score: ',f1_score(y_test, y_predict))

"""Same accuracy as before when we did not use tfidfVectorizer

I have used naive_bayes.MultinomialNB() model
"""